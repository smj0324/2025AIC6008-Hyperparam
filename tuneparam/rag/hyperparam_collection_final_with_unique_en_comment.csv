Paper Title,Model,Optimizer,Learning Rate,Dropout,Batch Size,Epochs,Momentum,Weight Decay,Activation,Hidden Layers,Hidden Units,Filter Size,Kernel Size,Beta1,Beta2,Epsilon,EMA Decay,L2-reg,Label Smoothing,Peak Stoch Depth Drop,RandAug Prob,RandAug Layers,RandAug Magnitude,RandAug Excl Ops,Mixup/Cutmix Prob,Mixup Alpha,Cutmix Alpha,Mix/Cut Switch Prob,Lag Time,Subsequences,Word Embeddings,Char Rep.,Tagging Scheme,Classifier,Gradient Normalization,Backend,RMSE,NSE,Tune Time (sec),# Estimators,Max Depth,Max Samples,Loss,Comment
High-speed hyperparameter optimization for deep ResNet models in image recognition,ResNet,SGD,0.02,0.3,128.0,200.0,0.9,0.0005,ReLU,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"ResNet: Fastest convergence setup for CIFAR-10, as reported in high-speed optimization experiments."
Hyperparameter Optimization for Deep Residual Learning in Image Classification,ResNet,SGD,0.04,0.5,128.0,200.0,0.9,0.0005,ReLU,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ResNet: Official recommended hyperparameters for large-scale image classification from optimization paper.
An Empirical Study of the Impact of Hyperparameter Tuning and Model Optimization on the Performance Properties of Deep Neural Networks,ResNet,RMSprop,0.001,0.2,256.0,100.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ResNet: Parameters offering the best trade-off between accuracy and training time (empirical study).
Recurrent Neural Network Tagging: Can We Beat the F1 Score?,LSTM,Nadam,,Variational (0.0~0.5),,,,,softmax/CRF,2.0,100.0,,,,,,,,,,,,,,,,,,,,,,,1~32,,Komninos/GloVe/FastText,CNN/LSTM,BIO/IOBES,CRF,Ï„=1,Theano/Tensorflow,,,LSTM: Variational dropout and CRF classifier achieved highest F1 for sequence labeling tasks.
Effects of Automatic Hyperparameter Tuning on the Performance of Multi-Variate Deep Learning-Based Rainfall Nowcasting,Stacked LSTM,,0.001,,1024.0,,,,,,3.0,30.0,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,LSTM (rainfall nowcasting): Auto-tuned parameters yielding lowest RMSE for 6-hour ahead forecasting.
Effects of Automatic Hyperparameter Tuning on the Performance of Multi-Variate Deep Learning-Based Rainfall Nowcasting,CNN,,0.0001,,64.0,,,,,,5.0,10.0,64.0,4,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,"CNN (rainfall nowcasting): Optimized for small batch size and deep architecture, tuned for time-series data."
Effects of Automatic Hyperparameter Tuning on the Performance of Multi-Variate Deep Learning-Based Rainfall Nowcasting,Bi-LSTM,,0.01,,128.0,,,,,,4.0,80.0,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,Bi-LSTM: Achieved highest NSE through auto-tuning for multivariate rainfall prediction.
Effects of Automatic Hyperparameter Tuning on the Performance of Multi-Variate Deep Learning-Based Rainfall Nowcasting,CNN-LSTM,,0.0001,0.1,2048.0,,,,,,3.0,70.0,52.0,2,,,,,,,,,,,,12.0,2.0,,,,,,,,,,,,,,,,,"CNN-LSTM: Combined CNN features with LSTM, Mixup and Cutmix, lowest RMSE in hybrid model."
Effects of Automatic Hyperparameter Tuning on the Performance of Multi-Variate Deep Learning-Based Rainfall Nowcasting,Conv-LSTM,,0.001,0.1,1024.0,,,,,,9.0,80.0,64.0,"(1,2)",,,,,,,,,,,10,2.0,,,,,,,,,,,,,,,,,,"Conv-LSTM: Maximized filter size and dropout, best for long sequence temporal modeling."
Effects of Automatic Hyperparameter Tuning on the Performance of Multi-Variate Deep Learning-Based Rainfall Nowcasting,CNN-Bi-LSTM,,0.01,0.5,1024.0,,,,,,5.0,30.0,28.0,1,,,,,,,,,,,,10.0,2.0,,,,,,,,,,,,,,,,,"CNN-Bi-LSTM: Large dropout and deep CNN, generalizes best in rainfall prediction tasks."
Effects of Automatic Hyperparameter Tuning on the Performance of Multi-Variate Deep Learning-Based Rainfall Nowcasting,Bagging,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.628,0.36,43,201,,,"Bagging ensemble: 201 estimators, tuned for lowest RMSE in ensemble setting (see Table 6)."
Effects of Automatic Hyperparameter Tuning on the Performance of Multi-Variate Deep Learning-Based Rainfall Nowcasting,Random Forest,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.573,0.38,48,583,8.0,,"Random Forest: Deepest tree (max depth=8), 583 estimators, best NSE among ensemble models."
Effects of Automatic Hyperparameter Tuning on the Performance of Multi-Variate Deep Learning-Based Rainfall Nowcasting,AdaBoost,,0.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.661,0.33,33,10,,,"AdaBoost: 10 estimators, square loss, high learning rate (0.5), optimized for fast convergence."
MobileNetV4 for ImageNet-1k Classification (Table 10),Conv-S,AdamW,0.002,0.3,4096.0,9600.0,,,,,,,,,0.6,0.999,1e-06,0.9999,1e-05,0.1,0.0,0.5,2.0,9.0,Cutout,,,,,,,,,,,,,,,,,,,"MobileNetV4 Conv-S: Smallest variant, large batch size, highest EMA decay, moderate augmentation."
MobileNetV4 for ImageNet-1k Classification (Table 10),Conv-M,AdamW,0.004,0.2,4096.0,500.0,,,,,,,,,0.9,0.999,1e-07,,,0.1,0.075,0.7,2.0,15.0,Cutout,,,,,,,,,,,,,,,,,,,"MobileNetV4 Conv-M: Medium Conv model, increased dropout, AdamW optimizer, balanced setup."
MobileNetV4 for ImageNet-1k Classification (Table 10),Hybrid-M,AdamW,0.016,0.2,16384.0,500.0,,,,,,,,,0.9,0.999,1e-07,,,0.1,0.075,0.7,2.0,15.0,Cutout,,,,,,,,,,,,,,,,,,,MobileNetV4 Hybrid-M: Hybrid architecture with largest batch and highest peak learning rate.
MobileNetV4 for ImageNet-1k Classification (Table 10),Conv-L,AdamW,0.004,0.2,16384.0,500.0,,,,,,,,,0.9,0.999,1e-07,,,0.1,0.35,1.0,2.0,15.0,Cutout,0.3,0.8,1.0,0.5,,,,,,,,,,,,,,,"MobileNetV4 Conv-L: Deepest convolutional variant, high stochastic depth and extensive mixup."
MobileNetV4 for ImageNet-1k Classification (Table 10),Hybrid-L,AdamW,0.01,0.2,16384.0,500.0,,,,,,,,,0.9,0.999,1e-07,,,0.1,0.35,1.0,2.0,15.0,Cutout,0.3,0.8,1.0,0.5,,,,,,,,,,,,,,,"MobileNetV4 Hybrid-L: Most aggressive regularization and augmentation, highest learning rate."
