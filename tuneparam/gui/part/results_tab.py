# ê·¸ë˜í”„ìš© ì‘ì€ í°íŠ¸ ì •ì˜
from tuneparam.framework.keras_ import TrainingLogger
from tkinter import ttk
from operator import itemgetter
import json

from tuneparam.model import HyperparameterOptimizer

GRAPH_FONT = ('Helvetica', 8)


def generate_hparam_prompt(current_params, training_results, dataset_type, goal, model_prompt):
    """
    í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜.

    Args:
        current_params (dict): í˜„ì¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •.
        training_results (dict or list): í•™ìŠµ ê²°ê³¼ ë¡œê·¸.
        dataset_type (str): ì˜ˆ: 'image_classification'.
        goal (str): ì˜ˆ: 'maximize validation accuracy'.
        model_prompt (str): ëª¨ë¸ ì„¤ëª… ë˜ëŠ” êµ¬ì„± ìš”ì•½.

    Returns:
        str: GPTì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´.
    """
    user_prompt = f"""
    Current hyperparameters:
    {json.dumps(current_params, indent=2)}

    Training results:
    {json.dumps(training_results, indent=2)}

    Dataset type: {dataset_type}
    Optimization goal: {goal}

    {model_prompt}

    Respond in this JSON format only:
    {{
      "recommendations": {{
        "learning_rate": value,
        "batch_size": value,
        "epochs": value,
        "optimizer": "value"
      }},
      "reasons": {{
        "learning_rate": "reason",
        "batch_size": "reason",
        "epochs": "reason",
        "optimizer": "reason"
      }},
      "expected_improvement": "description"
    }}
    """
    return user_prompt.strip()


def get_theme_colors(is_dark_theme=False):
    """í…Œë§ˆì— ë”°ë¥¸ ìƒ‰ìƒ ë°˜í™˜"""
    if is_dark_theme:
        return {
            'bg': "#2b2b2b",
            'fg': "#ffffff",  # ë” ë°ì€ í°ìƒ‰ìœ¼ë¡œ ë³€ê²½
            'grid': "#404040",
            'axis': "#ffffff",  # ì¶• ìƒ‰ìƒë„ ë” ë°ê²Œ
            'loss': "#ff4444",
            'val_loss': "#ffaa44",
            'acc': "#4a9eff",
            'val_acc': "#66ccff",
            'canvas_bg': "#2b2b2b"
        }
    else:
        return {
            'bg': "#ffffff",
            'fg': "#313131",
            'grid': "#f0f0f0",
            'axis': "#313131",
            'loss': "#ff0000",
            'val_loss': "#ff8800",
            'acc': "#0066cc",
            'val_acc': "#00aaff",
            'canvas_bg': "white"
        }

def split_summary_by_keys(summary, keys):
    getter = itemgetter(*keys)
    included = dict(zip(keys, getter(summary)))
    excluded = {k: v for k, v in summary.items() if k not in keys}
    return included, excluded


def apply_gpt_params(tab_train, logger: TrainingLogger):
    print("íŒŒë¼ë¯¸í„° ì¶”ì²œ ì‹œì‘")
    included, excluded = split_summary_by_keys(logger.summary, logger.params_key)
    print(included)
    print(excluded)
    optimizer = HyperparameterOptimizer()

    recommendations = optimizer.recommend_params(
        current_params=included,
        training_results=excluded,
        model_name="MobilenetV4",
        dataset_type="Image",
        goal="Accuracy"
    )
    print("Recommended hyperparameters:")
    print(json.dumps(recommendations, indent=2, ensure_ascii=False))

    # try:
    #     tab_train.lr_entry.delete(0, tk.END)
    #     tab_train.lr_entry.insert(0, "0.001")
    #
    #     tab_train.batch_entry.delete(0, tk.END)
    #     tab_train.batch_entry.insert(0, str(recommended_params['batch_size']))
    #
    #     tab_train.epochs_entry.delete(0, tk.END)
    #     tab_train.epochs_entry.insert(0, str(recommended_params['epochs']))
    # except AttributeError:
    #     print("Entry ìœ„ì ¯ì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

def on_retrain_with_params(train_parameters):
    print("ì´ ì„¤ì •ìœ¼ë¡œ ì¬í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤:")
    print("í›ˆë ¨ ì‹œì‘í•˜ê¸°")
    for key, val in train_parameters.items():
        print(f"  {key}: {val}")


def show_gpt_params(tab_train, logger: TrainingLogger):
    # ìš”ê¸°ì„œ
    original_params = logger.summary

    print(original_params)

    # ê¸°ì¡´ì— ìˆë˜ gpt_frame ì‚­ì œ (ì¤‘ë³µ ë°©ì§€)
    for child in tab_train.winfo_children():
        if getattr(child, "is_gpt_frame", False):
            child.destroy()

    gpt_frame = ttk.LabelFrame(tab_train, text="ğŸ“Œ LLM ì¶”ì²œ í•˜ì´í¼íŒŒë¼ë¯¸í„°")
    gpt_frame.is_gpt_frame = True
    gpt_frame.pack(side="left", fill="both", expand=True, padx=10, pady=10)

    for key, value in original_params.items():
        row = ttk.Frame(gpt_frame)
        row.pack(anchor="w", padx=10, pady=2)

        key_label = ttk.Label(row, text=f"{key}:", width=18)
        key_label.pack(side="left")

        value_label = ttk.Label(row, text=str(value))
        value_label.pack(side="left")

    apply_button = ttk.Button(
        gpt_frame,
        text="ì¶”ì²œê°’ ì ìš©í•˜ê¸°",
        command=lambda: apply_gpt_params(tab_train, logger)
    )
    apply_button.pack(pady=15)


def setup_results_tab(tab_train,  train_parameters=None, preset_logger : TrainingLogger = None):

    if train_parameters:
        used_params_frame = ttk.LabelFrame(tab_train, text="ì‚¬ìš©ëœ í›ˆë ¨ íŒŒë¼ë¯¸í„°")
        used_params_frame.pack(side="left", fill="both", expand=True, padx=10, pady=10)

        for key, value in train_parameters.items():
            row = ttk.Frame(used_params_frame)
            row.pack(anchor="w", padx=10, pady=2)

            key_label = ttk.Label(row, text=f"{key}:", width=18)
            key_label.pack(side="left")

            value_label = ttk.Label(row, text=str(value))
            value_label.pack(side="left")

        retrain_button = ttk.Button(
            used_params_frame,
            text="í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì²œë°›ê¸°",
            command=lambda: show_gpt_params(tab_train, preset_logger)
        )
        retrain_button.pack(pady=15)
